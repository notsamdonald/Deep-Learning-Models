{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5814: Assignment 2\n",
    "\n",
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\anaconda3\\envs\\transformers_2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Random Seed\n",
    "RANDOM_SEED = 3819969\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_loc='code_dataset.jsonl', generate_histogram=False):\n",
    "    \"\"\"\n",
    "    Loads and processing the jsonl file,\n",
    "\n",
    "    :param file_loc: location of target jsonl file\n",
    "    :param generate_histogram: Flag to display histogram of function lengths\n",
    "    :return: dataframe of preprocessed jsons\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_loc, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    code_list = []\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        code_list.append(result)\n",
    "\n",
    "    code_df = pd.DataFrame(code_list)\n",
    "\n",
    "    total = code_df['target'].sum()\n",
    "    proportion = total / code_df.shape[0]\n",
    "\n",
    "    print(\"Insecure code counts: {}, Total code counts: {}, Proportion {}\".format(total, code_df.shape[0], proportion))\n",
    "\n",
    "    if generate_histogram:\n",
    "        plt.hist(code_df['func'].str.len(), bins=100)\n",
    "        plt.show()\n",
    "\n",
    "    return code_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(input_data, attention_data, label_data, train_ratio=0.8, val_ratio=0.10, max_len=512):\n",
    "    \"\"\"\n",
    "    Splits data in accordance with provdied ratios, additionally discards functions with > max_len tokens\n",
    "        as these will not be processed by the model will (can truncate, yet may truncate the error in the code)\n",
    "\n",
    "    :param input_data: input functions\n",
    "    :param attention_data: attention map\n",
    "    :param label_data: target labels\n",
    "    :param train_ratio: ratio of data to train on\n",
    "    :param val_ratio: ratio of data to validate with (test is inferred from this and train)\n",
    "    :param max_len: max number of tokens allowed for training date\n",
    "\n",
    "    :return: 3 tuples for train val and test containing (input, attention, target)\n",
    "    \"\"\"\n",
    "    # Removing excessively long elements from dataset\n",
    "    valid_token_index = [i for i in range(len(input_data)) if len(input_data[i]) <= max_len]\n",
    "    X_data = np.array(input_data)[valid_token_index]\n",
    "    A_data = np.array(attention_data)[valid_token_index]\n",
    "    Y_data = np.array(label_data)[valid_token_index]\n",
    "\n",
    "    dataset_size = len(X_data)\n",
    "\n",
    "    # Determining index to split dataset\n",
    "    random_id = random.sample(range(dataset_size), dataset_size)\n",
    "    train_split_id = int(train_ratio * dataset_size)\n",
    "    val_split_id = int((train_ratio + val_ratio) * dataset_size)\n",
    "\n",
    "    train_ids = random_id[:train_split_id]\n",
    "    val_ids = random_id[train_split_id:val_split_id]\n",
    "    test_ids = random_id[val_split_id:]\n",
    "\n",
    "    X_train = torch.tensor(list(X_data[train_ids]))\n",
    "    A_train = torch.tensor(list(A_data[train_ids]))\n",
    "    Y_train = torch.tensor(list(Y_data[train_ids]))\n",
    "\n",
    "    X_val = torch.tensor(list(X_data[val_ids]))\n",
    "    A_val = torch.tensor(list(A_data[val_ids]))\n",
    "    Y_val = torch.tensor(list(Y_data[val_ids]))\n",
    "\n",
    "    X_test = torch.tensor(list(X_data[test_ids]))\n",
    "    A_test = torch.tensor(list(A_data[test_ids]))\n",
    "    Y_test = torch.tensor(list(Y_data[test_ids]))\n",
    "\n",
    "    return (X_train, A_train, Y_train), (X_val, A_val, Y_val), (X_test, A_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(code_df, model_name='codebert-base'):\n",
    "    \"\"\"\n",
    "    Apply the tokenizer from the huggingface pretrained model\n",
    "\n",
    "    :param code_df: dataframe of preprocess code (from jsonl)\n",
    "    :param model_name: model name (targeting local install)\n",
    "    :return: 3 tuples for train val and test containing (input, attention, target)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(code_df['func'].tolist(), truncation=False, padding='max_length')\n",
    "\n",
    "    input_data = inputs['input_ids']\n",
    "    attention_data = inputs['attention_mask']\n",
    "    label_data = torch.tensor(code_df['target'].tolist())  # TODO - this can be directly converted to a np array\n",
    "\n",
    "    return split_data(input_data, attention_data, label_data, max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, epochs=5, batch_size=16, learning_rate=2e-5, validate_per=500,\n",
    "          run_name=\"temp\", run_descrption=None):\n",
    "    \"\"\"\n",
    "    Main fine-tuning training loop for the provided model\n",
    "\n",
    "    :param model: model loaded with predefined weights\n",
    "    :param train_data: tuple of X_train, A_train, Y_train (X = inputs, A = attention, Y = target)\n",
    "    :param val_data: tuple X_val, A_val, Y_val\n",
    "    :param epochs: number of epochs for training\n",
    "    :param batch_size: batch size (see note below about batch_hack)\n",
    "    :param learning_rate: optimizer learning rate\n",
    "    :param validate_per: number of weight updates before validation occurs\n",
    "                            (notes: - if batch_size = 32, and validate_per = 32, validation will occur every batch\n",
    "                                    - this is wrt the start of each epoch\n",
    "                                    - validation will always occour at the start of each epoch (step 0))\n",
    "    :param run_name: name used to saving checkpoints and log files within codebert_finetune_runs\n",
    "    :param run_descrption: string that is saved to info.txt describing the run\n",
    "\n",
    "\n",
    "    :return: None (models are saved in checkpoints along with log data)\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating dir to save logs and checkpoints, re\n",
    "    dir_name = \"codebert_finetune_runs/{}\".format(run_name)\n",
    "    if os.path.exists(dir_name):\n",
    "        print(\"run name already exists, exiting to prevent overwriting\")\n",
    "        return 0\n",
    "    else:\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    # Saving run description.txt\n",
    "    if run_descrption is not None:\n",
    "        with open(\"{}/info.txt\".format(dir_name), \"a+\") as f:\n",
    "            f.write(run_descrption)\n",
    "\n",
    "    # Unpacking data\n",
    "    X_train, A_train, Y_train = train_data\n",
    "    X_val, A_val, Y_val = val_data\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_hack = batch_size  # See note below regarding limited GPU memory\n",
    "\n",
    "    # Initializing arrays for tracking loss\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "\n",
    "    # Counter to track batches (see note below related to GPU memory)\n",
    "    batch_count = 0\n",
    "    # validate_per_batch = int(validate_per/batch_hack)\n",
    "\n",
    "    # Moving model to GPU if configured\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Generating random index for manual shuffling of data each epoch as note using DataLoaders\n",
    "        permutation = torch.randperm(X_train.shape[0])\n",
    "\n",
    "        # Note here that only a single element is loaded at each iteration (batch size = 1) due to GPU memory constraint\n",
    "        for i in range(0, X_train.shape[0], 1):\n",
    "\n",
    "            # Loading batch and moving to device\n",
    "            indices = permutation[i:i + 1]\n",
    "            batch_X, batch_Y = X_train[indices].to(device), Y_train[indices].to(device), \\\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X, labels=batch_Y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Tracking loss\n",
    "            train_loss_hist.append(loss.item())\n",
    "\n",
    "            # Training output\n",
    "            train_output = \"Epoch:{} Step:{} Training_loss:{:.6f}\".format(epoch, i, loss.item())\n",
    "            print(train_output+\" Training_loss_avg:{:.6f}\".format(np.average(train_loss_hist[-50:])))\n",
    "            with open(\"{}/train_loss.txt\".format(dir_name), \"a+\") as f:\n",
    "                f.write(train_output+\"\\n\")\n",
    "\n",
    "            # Validation\n",
    "            if i % validate_per == 0:\n",
    "                val_loss_total = 0\n",
    "                model.eval()\n",
    "                print(\"Validating:\")\n",
    "                for j in tqdm(range(0, X_val.shape[0], 1)):\n",
    "                    # Loading singular validation data (overwrites train data as can only load 1 intp GPU)\n",
    "                    batch_X, batch_Y = X_val[j].to(device).reshape(1, -1), Y_val[j].to(device).reshape(1, -1)\n",
    "                    with torch.no_grad():\n",
    "                        val_outputs = model(batch_X, labels=batch_Y)\n",
    "                    val_loss_total += val_outputs['loss'].item()\n",
    "\n",
    "                # Adding average loss to tracker\n",
    "                val_average = val_loss_total / (X_val.shape[0])\n",
    "                val_loss_hist.append(val_average)\n",
    "\n",
    "                # Validation output and logging\n",
    "                val_output = \"Epoch:{} Step:{} Val_loss:{:.6f}\".format(epoch, i, val_average)\n",
    "                print(val_output)\n",
    "                with open(\"{}/val_los.txt\".format(dir_name), \"a+\") as f:\n",
    "                    f.write(val_output+\"\\n\")\n",
    "\n",
    "        # End of epoch checkpoint\n",
    "        model.save_pretrained(\"{}/epoch_{}\".format(dir_name, epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main configuration function for a given finetune run\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    run_name = \"test3\"\n",
    "    model_name = 'codebert-base'\n",
    "    checkpoint_location = None\n",
    "\n",
    "    code_df = preprocess_data(file_loc='code_dataset.jsonl')\n",
    "    train_data, val_data, test_data = tokenize(code_df, model_name=model_name)\n",
    "\n",
    "    # Loading model from checkpoint if location provided\n",
    "    if checkpoint_location is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_location)\n",
    "\n",
    "    train(model=model,\n",
    "          train_data=train_data,\n",
    "          val_data=val_data,\n",
    "          epochs=5,\n",
    "          batch_size=1,\n",
    "          learning_rate=1e-4,\n",
    "          validate_per=500,\n",
    "          run_name=run_name,\n",
    "          run_descrption=\"Fixed validation bug, lr=1e-4, validate per 500, batch 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insecure code counts: 3729, Total code counts: 8000, Proportion 0.466125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_17680\\1479780708.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_data = np.array(input_data)[valid_token_index]\n",
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_17680\\1479780708.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  A_data = np.array(attention_data)[valid_token_index]\n",
      "Some weights of the model checkpoint at codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Sam\\anaconda3\\envs\\transformers_2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:0 Training_loss:0.432128 Training_loss_avg:0.432128\n",
      "Validating:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 416/416 [00:35<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:0 Val_loss:0.891982\n",
      "Epoch:0 Step:1 Training_loss:0.210745 Training_loss_avg:0.321437\n",
      "Epoch:0 Step:2 Training_loss:2.511854 Training_loss_avg:1.051576\n",
      "Epoch:0 Step:3 Training_loss:0.122689 Training_loss_avg:0.819354\n",
      "Epoch:0 Step:4 Training_loss:0.130322 Training_loss_avg:0.681548\n",
      "Epoch:0 Step:5 Training_loss:1.907819 Training_loss_avg:0.885926\n",
      "Epoch:0 Step:6 Training_loss:1.492967 Training_loss_avg:0.972646\n",
      "Epoch:0 Step:7 Training_loss:0.482240 Training_loss_avg:0.911345\n",
      "Epoch:0 Step:8 Training_loss:0.586573 Training_loss_avg:0.875260\n",
      "Epoch:0 Step:9 Training_loss:0.628762 Training_loss_avg:0.850610\n",
      "Epoch:0 Step:10 Training_loss:0.569177 Training_loss_avg:0.825025\n",
      "Epoch:0 Step:11 Training_loss:0.559368 Training_loss_avg:0.802887\n",
      "Epoch:0 Step:12 Training_loss:0.386136 Training_loss_avg:0.770829\n",
      "Epoch:0 Step:13 Training_loss:0.275752 Training_loss_avg:0.735467\n",
      "Epoch:0 Step:14 Training_loss:1.955227 Training_loss_avg:0.816784\n",
      "Epoch:0 Step:15 Training_loss:0.216072 Training_loss_avg:0.779239\n",
      "Epoch:0 Step:16 Training_loss:0.214203 Training_loss_avg:0.746002\n",
      "Epoch:0 Step:17 Training_loss:1.419739 Training_loss_avg:0.783432\n",
      "Epoch:0 Step:18 Training_loss:0.277677 Training_loss_avg:0.756813\n",
      "Epoch:0 Step:19 Training_loss:1.035310 Training_loss_avg:0.770738\n",
      "Epoch:0 Step:20 Training_loss:0.332815 Training_loss_avg:0.749885\n",
      "Epoch:0 Step:21 Training_loss:0.287965 Training_loss_avg:0.728888\n",
      "Epoch:0 Step:22 Training_loss:0.251136 Training_loss_avg:0.708116\n",
      "Epoch:0 Step:23 Training_loss:1.578206 Training_loss_avg:0.744370\n",
      "Epoch:0 Step:24 Training_loss:1.446000 Training_loss_avg:0.772435\n",
      "Epoch:0 Step:25 Training_loss:0.324195 Training_loss_avg:0.755195\n",
      "Epoch:0 Step:26 Training_loss:1.147902 Training_loss_avg:0.769740\n",
      "Epoch:0 Step:27 Training_loss:0.841006 Training_loss_avg:0.772285\n",
      "Epoch:0 Step:28 Training_loss:0.677985 Training_loss_avg:0.769033\n",
      "Epoch:0 Step:29 Training_loss:0.574100 Training_loss_avg:0.762536\n",
      "Epoch:0 Step:30 Training_loss:0.669229 Training_loss_avg:0.759526\n",
      "Epoch:0 Step:31 Training_loss:0.632445 Training_loss_avg:0.755555\n",
      "Epoch:0 Step:32 Training_loss:0.741319 Training_loss_avg:0.755123\n",
      "Epoch:0 Step:33 Training_loss:0.579191 Training_loss_avg:0.749949\n",
      "Epoch:0 Step:34 Training_loss:0.977566 Training_loss_avg:0.756452\n",
      "Epoch:0 Step:35 Training_loss:1.019325 Training_loss_avg:0.763754\n",
      "Epoch:0 Step:36 Training_loss:0.602911 Training_loss_avg:0.759407\n",
      "Epoch:0 Step:37 Training_loss:0.818942 Training_loss_avg:0.760974\n",
      "Epoch:0 Step:38 Training_loss:0.746639 Training_loss_avg:0.760606\n",
      "Epoch:0 Step:39 Training_loss:0.629014 Training_loss_avg:0.757316\n",
      "Epoch:0 Step:40 Training_loss:0.539794 Training_loss_avg:0.752011\n",
      "Epoch:0 Step:41 Training_loss:0.529824 Training_loss_avg:0.746721\n",
      "Epoch:0 Step:42 Training_loss:0.678566 Training_loss_avg:0.745136\n",
      "Epoch:0 Step:43 Training_loss:0.271721 Training_loss_avg:0.734376\n",
      "Epoch:0 Step:44 Training_loss:0.269470 Training_loss_avg:0.724045\n",
      "Epoch:0 Step:45 Training_loss:2.153076 Training_loss_avg:0.755111\n",
      "Epoch:0 Step:46 Training_loss:0.105443 Training_loss_avg:0.741288\n",
      "Epoch:0 Step:47 Training_loss:0.232514 Training_loss_avg:0.730689\n",
      "Epoch:0 Step:48 Training_loss:0.213345 Training_loss_avg:0.720131\n",
      "Epoch:0 Step:49 Training_loss:0.205218 Training_loss_avg:0.709832\n",
      "Epoch:0 Step:50 Training_loss:1.842701 Training_loss_avg:0.738044\n",
      "Epoch:0 Step:51 Training_loss:0.156795 Training_loss_avg:0.736965\n",
      "Epoch:0 Step:52 Training_loss:1.883100 Training_loss_avg:0.724390\n",
      "Epoch:0 Step:53 Training_loss:0.252235 Training_loss_avg:0.726981\n",
      "Epoch:0 Step:54 Training_loss:1.510347 Training_loss_avg:0.754581\n",
      "Epoch:0 Step:55 Training_loss:1.300403 Training_loss_avg:0.742433\n",
      "Epoch:0 Step:56 Training_loss:0.525036 Training_loss_avg:0.723074\n",
      "Epoch:0 Step:57 Training_loss:0.603382 Training_loss_avg:0.725497\n",
      "Epoch:0 Step:58 Training_loss:0.762275 Training_loss_avg:0.729011\n",
      "Epoch:0 Step:59 Training_loss:0.804053 Training_loss_avg:0.732517\n",
      "Epoch:0 Step:60 Training_loss:0.500281 Training_loss_avg:0.731139\n",
      "Epoch:0 Step:61 Training_loss:0.915803 Training_loss_avg:0.738268\n",
      "Epoch:0 Step:62 Training_loss:0.446497 Training_loss_avg:0.739475\n",
      "Epoch:0 Step:63 Training_loss:1.062373 Training_loss_avg:0.755207\n",
      "Epoch:0 Step:64 Training_loss:1.075941 Training_loss_avg:0.737622\n",
      "Epoch:0 Step:65 Training_loss:0.480745 Training_loss_avg:0.742915\n",
      "Epoch:0 Step:66 Training_loss:0.879161 Training_loss_avg:0.756214\n",
      "Epoch:0 Step:67 Training_loss:0.547770 Training_loss_avg:0.738775\n",
      "Epoch:0 Step:68 Training_loss:0.863177 Training_loss_avg:0.750485\n",
      "Epoch:0 Step:69 Training_loss:0.630565 Training_loss_avg:0.742390\n",
      "Epoch:0 Step:70 Training_loss:0.733122 Training_loss_avg:0.750396\n",
      "Epoch:0 Step:71 Training_loss:0.714890 Training_loss_avg:0.758935\n",
      "Epoch:0 Step:72 Training_loss:0.692858 Training_loss_avg:0.767769\n",
      "Epoch:0 Step:73 Training_loss:0.720584 Training_loss_avg:0.750617\n",
      "Epoch:0 Step:74 Training_loss:0.652545 Training_loss_avg:0.734748\n",
      "Epoch:0 Step:75 Training_loss:0.660225 Training_loss_avg:0.741468\n",
      "Epoch:0 Step:76 Training_loss:0.657862 Training_loss_avg:0.731667\n",
      "Epoch:0 Step:77 Training_loss:0.834945 Training_loss_avg:0.731546\n",
      "Epoch:0 Step:78 Training_loss:0.820592 Training_loss_avg:0.734398\n",
      "Epoch:0 Step:79 Training_loss:0.857855 Training_loss_avg:0.740073\n",
      "Epoch:0 Step:80 Training_loss:0.574813 Training_loss_avg:0.738185\n",
      "Epoch:0 Step:81 Training_loss:0.579119 Training_loss_avg:0.737119\n",
      "Epoch:0 Step:82 Training_loss:0.558341 Training_loss_avg:0.733459\n",
      "Epoch:0 Step:83 Training_loss:0.541804 Training_loss_avg:0.732711\n",
      "Epoch:0 Step:84 Training_loss:0.504050 Training_loss_avg:0.723241\n",
      "Epoch:0 Step:85 Training_loss:0.489123 Training_loss_avg:0.712637\n",
      "Epoch:0 Step:86 Training_loss:0.445494 Training_loss_avg:0.709489\n",
      "Epoch:0 Step:87 Training_loss:0.388640 Training_loss_avg:0.700883\n",
      "Epoch:0 Step:88 Training_loss:1.855680 Training_loss_avg:0.723063\n",
      "Epoch:0 Step:89 Training_loss:0.387362 Training_loss_avg:0.718230\n",
      "Epoch:0 Step:90 Training_loss:0.379517 Training_loss_avg:0.715025\n",
      "Epoch:0 Step:91 Training_loss:0.142430 Training_loss_avg:0.707277\n",
      "Epoch:0 Step:92 Training_loss:0.348528 Training_loss_avg:0.700676\n",
      "Epoch:0 Step:93 Training_loss:1.260702 Training_loss_avg:0.720456\n",
      "Epoch:0 Step:94 Training_loss:0.350046 Training_loss_avg:0.722067\n",
      "Epoch:0 Step:95 Training_loss:0.323940 Training_loss_avg:0.685485\n",
      "Epoch:0 Step:96 Training_loss:1.281175 Training_loss_avg:0.708999\n",
      "Epoch:0 Step:97 Training_loss:0.323824 Training_loss_avg:0.710825\n",
      "Epoch:0 Step:98 Training_loss:0.326136 Training_loss_avg:0.713081\n",
      "Epoch:0 Step:99 Training_loss:1.192889 Training_loss_avg:0.732835\n",
      "Epoch:0 Step:100 Training_loss:0.375698 Training_loss_avg:0.703495\n",
      "Epoch:0 Step:101 Training_loss:1.676997 Training_loss_avg:0.733899\n",
      "Epoch:0 Step:102 Training_loss:1.174013 Training_loss_avg:0.719717\n",
      "Epoch:0 Step:103 Training_loss:1.069897 Training_loss_avg:0.736070\n",
      "Epoch:0 Step:104 Training_loss:0.943023 Training_loss_avg:0.724724\n",
      "Epoch:0 Step:105 Training_loss:0.802176 Training_loss_avg:0.714759\n",
      "Epoch:0 Step:106 Training_loss:0.710387 Training_loss_avg:0.718466\n",
      "Epoch:0 Step:107 Training_loss:0.614197 Training_loss_avg:0.718682\n",
      "Epoch:0 Step:108 Training_loss:0.531617 Training_loss_avg:0.714069\n",
      "Epoch:0 Step:109 Training_loss:0.463051 Training_loss_avg:0.707249\n",
      "Epoch:0 Step:110 Training_loss:1.038162 Training_loss_avg:0.718007\n",
      "Epoch:0 Step:111 Training_loss:1.205971 Training_loss_avg:0.723810\n",
      "Epoch:0 Step:112 Training_loss:0.379243 Training_loss_avg:0.722465\n",
      "Epoch:0 Step:113 Training_loss:1.135595 Training_loss_avg:0.723930\n",
      "Epoch:0 Step:114 Training_loss:1.186522 Training_loss_avg:0.726141\n",
      "Epoch:0 Step:115 Training_loss:0.363963 Training_loss_avg:0.723806\n",
      "Epoch:0 Step:116 Training_loss:0.417896 Training_loss_avg:0.714580\n",
      "Epoch:0 Step:117 Training_loss:0.408341 Training_loss_avg:0.711792\n",
      "Epoch:0 Step:118 Training_loss:1.118440 Training_loss_avg:0.716897\n",
      "Epoch:0 Step:119 Training_loss:1.036279 Training_loss_avg:0.725011\n",
      "Epoch:0 Step:120 Training_loss:1.087609 Training_loss_avg:0.732101\n",
      "Epoch:0 Step:121 Training_loss:2.122557 Training_loss_avg:0.760254\n",
      "Epoch:0 Step:122 Training_loss:0.856730 Training_loss_avg:0.763532\n",
      "Epoch:0 Step:123 Training_loss:0.817294 Training_loss_avg:0.765466\n",
      "Epoch:0 Step:124 Training_loss:0.726357 Training_loss_avg:0.766942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:125 Training_loss:0.659448 Training_loss_avg:0.766927\n",
      "Epoch:0 Step:126 Training_loss:0.848518 Training_loss_avg:0.770740\n",
      "Epoch:0 Step:127 Training_loss:0.922362 Training_loss_avg:0.772488\n",
      "Epoch:0 Step:128 Training_loss:0.510374 Training_loss_avg:0.766284\n",
      "Epoch:0 Step:129 Training_loss:1.050969 Training_loss_avg:0.770146\n",
      "Epoch:0 Step:130 Training_loss:0.457420 Training_loss_avg:0.767798\n",
      "Epoch:0 Step:131 Training_loss:1.046237 Training_loss_avg:0.777140\n",
      "Epoch:0 Step:132 Training_loss:1.067309 Training_loss_avg:0.787320\n",
      "Epoch:0 Step:133 Training_loss:0.402990 Training_loss_avg:0.784544\n",
      "Epoch:0 Step:134 Training_loss:0.447087 Training_loss_avg:0.783404\n",
      "Epoch:0 Step:135 Training_loss:1.121124 Training_loss_avg:0.796044\n",
      "Epoch:0 Step:136 Training_loss:0.396352 Training_loss_avg:0.795061\n",
      "Epoch:0 Step:137 Training_loss:1.045407 Training_loss_avg:0.808197\n",
      "Epoch:0 Step:138 Training_loss:1.087401 Training_loss_avg:0.792831\n",
      "Epoch:0 Step:139 Training_loss:1.352401 Training_loss_avg:0.812132\n",
      "Epoch:0 Step:140 Training_loss:0.424374 Training_loss_avg:0.813029\n",
      "Epoch:0 Step:141 Training_loss:0.477678 Training_loss_avg:0.819734\n",
      "Epoch:0 Step:142 Training_loss:0.986528 Training_loss_avg:0.832494\n",
      "Epoch:0 Step:143 Training_loss:0.904335 Training_loss_avg:0.825367\n",
      "Epoch:0 Step:144 Training_loss:0.515659 Training_loss_avg:0.828679\n",
      "Epoch:0 Step:145 Training_loss:0.560567 Training_loss_avg:0.833412\n",
      "Epoch:0 Step:146 Training_loss:0.583594 Training_loss_avg:0.819460\n",
      "Epoch:0 Step:147 Training_loss:0.612979 Training_loss_avg:0.825243\n",
      "Epoch:0 Step:148 Training_loss:0.571475 Training_loss_avg:0.830150\n",
      "Epoch:0 Step:149 Training_loss:0.615325 Training_loss_avg:0.818599\n",
      "Epoch:0 Step:150 Training_loss:0.641493 Training_loss_avg:0.823915\n",
      "Epoch:0 Step:151 Training_loss:0.809512 Training_loss_avg:0.806565\n",
      "Epoch:0 Step:152 Training_loss:0.543935 Training_loss_avg:0.793963\n",
      "Epoch:0 Step:153 Training_loss:0.585753 Training_loss_avg:0.784280\n",
      "Epoch:0 Step:154 Training_loss:0.647451 Training_loss_avg:0.778369\n",
      "Epoch:0 Step:155 Training_loss:0.701048 Training_loss_avg:0.776346\n",
      "Epoch:0 Step:156 Training_loss:0.632573 Training_loss_avg:0.774790\n",
      "Epoch:0 Step:157 Training_loss:0.650848 Training_loss_avg:0.775523\n",
      "Epoch:0 Step:158 Training_loss:0.657649 Training_loss_avg:0.778044\n",
      "Epoch:0 Step:159 Training_loss:0.686257 Training_loss_avg:0.782508\n",
      "Epoch:0 Step:160 Training_loss:0.695183 Training_loss_avg:0.775648\n",
      "Epoch:0 Step:161 Training_loss:0.768083 Training_loss_avg:0.766891\n",
      "Epoch:0 Step:162 Training_loss:0.627248 Training_loss_avg:0.771851\n",
      "Epoch:0 Step:163 Training_loss:0.732850 Training_loss_avg:0.763796\n",
      "Epoch:0 Step:164 Training_loss:0.728854 Training_loss_avg:0.754642\n",
      "Epoch:0 Step:165 Training_loss:0.320284 Training_loss_avg:0.753769\n",
      "Epoch:0 Step:166 Training_loss:0.666265 Training_loss_avg:0.758736\n",
      "Epoch:0 Step:167 Training_loss:0.737102 Training_loss_avg:0.765311\n",
      "Epoch:0 Step:168 Training_loss:0.689774 Training_loss_avg:0.756738\n",
      "Epoch:0 Step:169 Training_loss:0.681946 Training_loss_avg:0.749651\n",
      "Epoch:0 Step:170 Training_loss:0.723169 Training_loss_avg:0.742363\n",
      "Epoch:0 Step:171 Training_loss:0.703046 Training_loss_avg:0.713972\n",
      "Epoch:0 Step:172 Training_loss:0.670540 Training_loss_avg:0.710249\n",
      "Epoch:0 Step:173 Training_loss:0.671555 Training_loss_avg:0.707334\n",
      "Epoch:0 Step:174 Training_loss:0.756472 Training_loss_avg:0.707936\n",
      "Epoch:0 Step:175 Training_loss:0.708701 Training_loss_avg:0.708921\n",
      "Epoch:0 Step:176 Training_loss:0.705532 Training_loss_avg:0.706061\n",
      "Epoch:0 Step:177 Training_loss:0.698495 Training_loss_avg:0.701584\n",
      "Epoch:0 Step:178 Training_loss:0.718723 Training_loss_avg:0.705751\n",
      "Epoch:0 Step:179 Training_loss:0.695681 Training_loss_avg:0.698645\n",
      "Epoch:0 Step:180 Training_loss:1.389105 Training_loss_avg:0.717279\n",
      "Epoch:0 Step:181 Training_loss:0.718428 Training_loss_avg:0.710723\n",
      "Epoch:0 Step:182 Training_loss:1.122605 Training_loss_avg:0.711829\n",
      "Epoch:0 Step:183 Training_loss:1.069608 Training_loss_avg:0.725161\n",
      "Epoch:0 Step:184 Training_loss:0.533392 Training_loss_avg:0.726887\n",
      "Epoch:0 Step:185 Training_loss:0.933634 Training_loss_avg:0.723137\n",
      "Epoch:0 Step:186 Training_loss:0.468042 Training_loss_avg:0.724571\n",
      "Epoch:0 Step:187 Training_loss:0.427602 Training_loss_avg:0.712215\n",
      "Epoch:0 Step:188 Training_loss:0.312321 Training_loss_avg:0.696714\n",
      "Epoch:0 Step:189 Training_loss:0.355841 Training_loss_avg:0.676782\n",
      "Epoch:0 Step:190 Training_loss:0.155243 Training_loss_avg:0.671400\n",
      "Epoch:0 Step:191 Training_loss:0.288701 Training_loss_avg:0.667620\n",
      "Epoch:0 Step:192 Training_loss:1.455747 Training_loss_avg:0.677005\n",
      "Epoch:0 Step:193 Training_loss:2.842110 Training_loss_avg:0.715760\n",
      "Epoch:0 Step:194 Training_loss:0.094030 Training_loss_avg:0.707327\n",
      "Epoch:0 Step:195 Training_loss:0.242769 Training_loss_avg:0.700971\n",
      "Epoch:0 Step:196 Training_loss:1.359625 Training_loss_avg:0.716492\n",
      "Epoch:0 Step:197 Training_loss:0.281740 Training_loss_avg:0.709867\n",
      "Epoch:0 Step:198 Training_loss:1.498100 Training_loss_avg:0.728400\n",
      "Epoch:0 Step:199 Training_loss:0.294646 Training_loss_avg:0.721986\n",
      "Epoch:0 Step:200 Training_loss:0.316534 Training_loss_avg:0.715487\n",
      "Epoch:0 Step:201 Training_loss:1.331538 Training_loss_avg:0.725928\n",
      "Epoch:0 Step:202 Training_loss:0.276409 Training_loss_avg:0.720577\n",
      "Epoch:0 Step:203 Training_loss:1.308995 Training_loss_avg:0.735042\n",
      "Epoch:0 Step:204 Training_loss:0.323161 Training_loss_avg:0.728556\n",
      "Epoch:0 Step:205 Training_loss:0.378801 Training_loss_avg:0.722111\n",
      "Epoch:0 Step:206 Training_loss:1.171606 Training_loss_avg:0.732892\n",
      "Epoch:0 Step:207 Training_loss:0.397609 Training_loss_avg:0.727827\n",
      "Epoch:0 Step:208 Training_loss:0.364303 Training_loss_avg:0.721960\n",
      "Epoch:0 Step:209 Training_loss:0.341937 Training_loss_avg:0.715074\n",
      "Epoch:0 Step:210 Training_loss:0.213310 Training_loss_avg:0.705436\n",
      "Epoch:0 Step:211 Training_loss:0.345018 Training_loss_avg:0.696975\n",
      "Epoch:0 Step:212 Training_loss:0.325157 Training_loss_avg:0.690933\n",
      "Epoch:0 Step:213 Training_loss:0.319757 Training_loss_avg:0.682671\n",
      "Epoch:0 Step:214 Training_loss:0.376683 Training_loss_avg:0.675628\n",
      "Epoch:0 Step:215 Training_loss:0.326258 Training_loss_avg:0.675747\n",
      "Epoch:0 Step:216 Training_loss:1.394562 Training_loss_avg:0.690313\n",
      "Epoch:0 Step:217 Training_loss:1.464179 Training_loss_avg:0.704855\n",
      "Epoch:0 Step:218 Training_loss:1.377607 Training_loss_avg:0.718611\n",
      "Epoch:0 Step:219 Training_loss:0.137290 Training_loss_avg:0.707718\n",
      "Epoch:0 Step:220 Training_loss:0.345011 Training_loss_avg:0.700155\n",
      "Epoch:0 Step:221 Training_loss:0.334905 Training_loss_avg:0.692792\n",
      "Epoch:0 Step:222 Training_loss:0.324555 Training_loss_avg:0.685873\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_2]",
   "language": "python",
   "name": "conda-env-transformers_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
