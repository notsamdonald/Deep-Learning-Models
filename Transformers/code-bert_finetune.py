# -*- coding: utf-8 -*-
"""HW3_Q3_SamDonald.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11E3mizQ_72IVr1giluJL4c6lVA147q8a

# CS5814: Assignment 3

## Problem 3

In problem involves creating a defect detection model that can automatically detect whether a code is insecure and may attack software systems.

Work is centered on finetuning the pre-trained code-based models provided by Hugging Face package, by using the provided code_dataset.jsonl dataset.

## Installs
"""

pip install transformers

pip install xlsxwriter

"""## Imports"""

import json
import os
import random
import gc
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel
import pickle
import torch.nn as nn

torch.cuda.empty_cache()
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

"""## Configure Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My Drive/CS5814/HW3/Q3

"""## Preprocessing functions"""

def preprocess_data(file_loc='code_dataset.jsonl'):
    """
    Loads and processing the jsonl file,

    :param file_loc: location of target jsonl file
    :param generate_histogram: Flag to display histogram of function lengths
    :return: dataframe of preprocessed jsons
    """

    with open(file_loc, 'r') as json_file:
        json_list = list(json_file)

    code_list = []
    for json_str in json_list:
        result = json.loads(json_str)
        code_list.append(result)

    code_df = pd.DataFrame(code_list)

    total = code_df['target'].sum()
    proportion = total / code_df.shape[0]

    print("Insecure code counts: {}, Total code counts: {}, Proportion {}".format(total, code_df.shape[0], proportion))
        
    return code_df

def split_data(input_data, attention_data, label_data, train_ratio=0.8, val_ratio=0.10, max_len=512):
    """
    Splits data in accordance with provdied ratios, additionally discards functions with > max_len tokens
        as these will not be processed by the model will (can truncate, yet may truncate the error in the code)

    :param input_data: input functions
    :param attention_data: attention map
    :param label_data: target labels
    :param train_ratio: ratio of data to train on
    :param val_ratio: ratio of data to validate with (test is inferred from this and train)
    :param max_len: max number of tokens allowed for training date

    :return: 3 tuples for train val and test containing (input, attention, target)
    """
    # Removing excessively long elements from dataset
    valid_token_index = [i for i in range(len(input_data)) if len(input_data[i]) <= max_len]
    X_data = np.array(input_data)[valid_token_index]
    A_data = np.array(attention_data)[valid_token_index]
    Y_data = np.array(label_data)[valid_token_index]

    dataset_size = len(X_data)

    # Determining index to split dataset
    random_id = random.sample(range(dataset_size), dataset_size)
    train_split_id = int(train_ratio * dataset_size)
    val_split_id = int((train_ratio + val_ratio) * dataset_size)

    train_ids = random_id[:train_split_id]
    val_ids = random_id[train_split_id:val_split_id]
    test_ids = random_id[val_split_id:]

    X_train = torch.tensor(list(X_data[train_ids]))
    A_train = torch.tensor(list(A_data[train_ids]))
    Y_train = torch.tensor(list(Y_data[train_ids]))

    X_val = torch.tensor(list(X_data[val_ids]))
    A_val = torch.tensor(list(A_data[val_ids]))
    Y_val = torch.tensor(list(Y_data[val_ids]))

    X_test = torch.tensor(list(X_data[test_ids]))
    A_test = torch.tensor(list(A_data[test_ids]))
    Y_test = torch.tensor(list(Y_data[test_ids]))

    return (X_train, A_train, Y_train), (X_val, A_val, Y_val), (X_test, A_test, Y_test)

def tokenize(code_df, model_name='codebert-base'):
    """
    Apply the tokenizer from the huggingface pretrained model

    :param code_df: dataframe of preprocess code (from jsonl)
    :param model_name: model name (targeting local install)
    :return: 3 tuples for train val and test containing (input, attention, target)
    """
    #tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer = RobertaTokenizer.from_pretrained(model_name)

    inputs = tokenizer(code_df['func'].tolist(), truncation=False, padding='max_length', max_length=512)

    input_data = inputs['input_ids']
    attention_data = inputs['attention_mask']
    label_data = torch.tensor(code_df['target'].tolist())  # TODO - this can be directly converted to a np array

    print("Data points: {}".format(len(input_data)))

    return split_data(input_data, attention_data, label_data, max_len=512)

def split_loader(run_dir):
    data_type = ['train', 'val', 'test']
    data_split_type = ['X', 'A', 'Y']

    split_list = []

    for data_type_id in data_type:
      for split_type in data_split_type:
          with open('{}/{}_{}.pickle'.format(run_dir,data_type_id, split_type), 'rb') as input_file:
            object_file = pickle.load(input_file)
          split_list.append(object_file)
    X_train, A_train, Y_train, X_val, A_val, Y_val, X_test, A_test, Y_test = split_list

    return (X_train, A_train, Y_train), (X_val, A_val, Y_val), (X_test, A_test, Y_test)

"""## Training loop"""

def train(model, train_data, val_data, epochs=5, batch_size=16, learning_rate=2e-5, validate_per=500,
          dir_name="temp", run_descrption=None):
    """
    Main fine-tuning training loop for the provided model

    :param model: model loaded with predefined weights
    :param train_data: tuple of X_train, A_train, Y_train (X = inputs, A = attention, Y = target)
    :param val_data: tuple X_val, A_val, Y_val
    :param epochs: number of epochs for training
    :param batch_size: batch size (see note below about batch_hack)
    :param learning_rate: optimizer learning rate
    :param validate_per: number of weight updates before validation occurs
                            (notes: - if batch_size = 32, and validate_per = 32, validation will occur every batch
                                    - this is wrt the start of each epoch
                                    - validation will always occour at the start of each epoch (step 0))
    :param run_name: name used to saving checkpoints and log files within codebert_finetune_runs
    :param run_descrption: string that is saved to info.txt describing the run


    :return: None (models are saved in checkpoints along with log data)
    """

    # Saving run description.txt
    if run_descrption is not None:
        with open("{}/info.txt".format(dir_name), "w") as f:
            f.write(run_descrption)

    # Unpacking data
    X_train, A_train, Y_train = train_data
    X_val, A_val, Y_val = val_data


    batch_hack = batch_size  # See note below regarding limited GPU memory

    # Initializing arrays for tracking loss
    train_loss_hist = []
    val_loss_hist = []
    train_pred_hist = []
    # Counter to track batches (see note below related to GPU memory)
    batch_count = 0
    # validate_per_batch = int(validate_per/batch_hack)

    # Moving model to GPU if configured
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    validate_per = int(validate_per/batch_size)

    for epoch in range(epochs):

        # Generating random index for manual shuffling of data each epoch as note using DataLoaders
        permutation = torch.randperm(X_train.shape[0])

        # Note here that only a single element is loaded at each iteration (batch size = 1) due to GPU memory constraint
        for batch_id, i in enumerate(range(0, X_train.shape[0], batch_hack)):

            # Loading batch and moving to device
            indices = permutation[i:i + batch_hack]
            batch_X, batch_Y, batch_A = X_train[indices].to(device), Y_train[indices].to(device), A_train[indices].to(device)

            model.train()

            # Forward pass
            outputs = model(batch_X,labels=batch_Y, attention_mask=batch_A)
            #loss = criterion(loss_clsf.float(), batch_Y_one_hot.float())
            loss = outputs.loss
            #print(loss)

            # Backward pass
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            loss_clsf = nn.Softmax(dim=1)(outputs.logits)
            acc = np.average(torch.eq(batch_Y.cpu(), loss_clsf.argmax(axis=1).cpu()))
            #rint(correct)

            # Tracking loss
            train_loss_hist.append(float(loss.item()))
            train_pred_hist.append(acc)

            # Training output
            train_output = "Epoch:{} Step:{} Training_loss:{:.6f}, Acc_avg:{:.2f}%".format(epoch, i, loss.item(), np.sum(100*train_pred_hist[-50:])/min(len(train_pred_hist), 50))
            print(train_output+" Training_loss_avg:{:.6f}".format(np.average(train_loss_hist[-50:])))
            with open("{}/train_loss.txt".format(dir_name), "a+") as f:
                f.write(train_output+"\n")

            # Validation
            if batch_id % validate_per == 0:
                val_loss_total = 0
                model.eval()
                print("Validating:")
                val_acc = []
                for val_badtch_id, j in tqdm(enumerate(range(0, X_val.shape[0], batch_hack))):
                    # Loading singular validation data (overwrites train data as can only load 1 intp GPU)
                    batch_X, batch_Y, batch_A = X_val[j:j+batch_hack].to(device), Y_val[j:j+batch_hack].to(device), A_val[j:j+batch_hack].to(device)

                    with torch.no_grad():
                        val_outputs = model(batch_X, labels=batch_Y, attention_mask=batch_A)
                    val_loss_total += float(val_outputs['loss'].item())

                    
                    val_clsf = nn.Softmax(dim=1)(val_outputs.logits)
                    val_acc.append(np.average(torch.eq(batch_Y.cpu(), val_clsf.argmax(axis=1).cpu())))

                    del batch_X
                    del batch_Y

                # Adding average loss to tracker
                val_average = val_loss_total / (val_badtch_id+1)
                val_loss_hist.append(val_average)

                # Validation output and logging
                val_output = "Epoch:{} Step:{} Val_loss:{:.6f}, Val_Acc_avg:{:.2f}%".format(epoch, i, val_average, np.sum(100*val_acc[-50:])/min(len(val_acc), 50))
                print(val_output)
                with open("{}/val_los.txt".format(dir_name), "a+") as f:
                    f.write(val_output+"\n")

        # End of epoch checkpoint
        model.save_pretrained("{}/epoch_{}".format(dir_name, epoch + 1))

"""## Analysing code length, token lengths, and sample tokens (part c and d)

"""

# NOTE that this is done on a per character basis within the function!

code_df = preprocess_data(file_loc='code_dataset.jsonl')
func_len = list(code_df['func'].str.len())

print(func_len[0])
func_len.sort()

plt.hist(func_len, bins=400, range=(0,16000))
plt.xlabel("Function length")
plt.ylabel("Frequency")
plt.show()

plt.hist(func_len, bins=100, range=(0,4000))
plt.xlabel("Function length")
plt.ylabel("Frequency")
plt.show()

ratio_1000 = sum(i < 1000 for i in func_len)/len(func_len)
print("\n{}% of functions are of shorter than 1000 characters".format(100*ratio_1000))

model_name = 'codebert-base'
tokenizer = RobertaTokenizer.from_pretrained(model_name)
inputs = tokenizer(code_df['func'].tolist(), truncation=False)
input_data = inputs['input_ids']

data_len = []
for data in input_data:
  data_len.append(len(data))

data_len.sort()
plt.hist(data_len, bins=100, range=(0,2000))
plt.xlabel("Function length")
plt.ylabel("Frequency")
plt.show()

ratio_512 = sum(i < 512 for i in data_len)/len(data_len)
print("{:.4f}% of tokenized functions are of shorter than 512 tokens".format(100*ratio_512))

model_name = 'codebert-base'
code_df = preprocess_data(file_loc='code_dataset.jsonl')
train_data, val_data, test_data = tokenize(code_df, model_name=model_name)

train_input, train_attention, _ = train_data

for i in range(10):
  print("Sample: {}".format(i+1))
  print("Input Data")
  print(train_input[i])
  print("Attention Mask")
  print(train_attention[i])

"""## Main training configuration and running model"""

def main():
    """
    Main configuration function for a given finetune run
    :return: None
    """

    run_name = "Final_lr=5e-6"
    model_name = 'codebert-base'
    dir_name = 'codebert_finetune_runs/{}'.format(run_name)

    checkpoint_location = None
    save_data = True
    online = False

    code_df = preprocess_data(file_loc='code_dataset.jsonl')
    train_data, val_data, test_data = tokenize(code_df, model_name=model_name)


    # Creating dir to save logs and checkpoints, re
    if os.path.exists(dir_name):
      input("run name already exists, press Enter to overwrite")
    else:
      os.makedirs(dir_name)

    if save_data:
      print("saving data splits")
      X_train, A_train, Y_train = train_data
      X_val, A_val, Y_val = val_data
      X_test, A_test, Y_test = test_data
      data_type = ['train', 'val', 'test']
      data_split_type = ['X', 'A', 'Y']
      data_all = [train_data, val_data, test_data]
      for i, data in enumerate(data_all):
        for j, split in enumerate(data):
          with open('{}/{}_{}.pickle'.format(dir_name,data_type[i], data_split_type[j]), 'wb') as handle:
            pickle.dump(split, handle)

    # Loading model from checkpoint if location provided
    if online:
        model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base")
    elif checkpoint_location is None:
        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    else:
        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_location)

    train(model=model,
          train_data=train_data,
          val_data=val_data,
          epochs=5,
          batch_size=8,
          learning_rate=5e-6,
          validate_per=100,
          dir_name=dir_name,
          run_descrption="Colab with highRam, lr=5e-6, validate per 100, batch 8, redo")

main()

"""## Post Training Analysis

### Training and validation loss vs epochs plot
"""

# Messy code to extract information from the dumped .txt files
# Could be significantly improved + streamlined

def moving_average(x, w):
    return np.convolve(x, np.ones(w), 'valid') / w

with open("{}/train_loss.txt".format(dir_name)) as f:
    contents = f.readlines()

train_loss = [i.split(' ')[-2].split(':')[-1] for i in contents]
train_loss = [float(i.split(',')[0]) for i in train_loss]

train_step = [int(i.split(' ')[1].split(':')[-1]) for i in contents]
train_epoch = [int(i.split(' ')[0].split(':')[-1]) for i in contents]

train_total_step = []

steps_per_epoch = max(train_step)
for i in range(len(train_step)):
    train_total_step.append(steps_per_epoch*train_epoch[i]+train_step[i])



with open("{}/val_loss.txt".format(dir_name)) as f:
    contents = f.readlines()

val_loss = [i.split(' ')[-2].split(':')[-1] for i in contents]
val_loss = [float(i.split(',')[0]) for i in val_loss]

val_step = [int(i.split(' ')[1].split(':')[-1]) for i in contents]
val_epoch = [int(i.split(' ')[0].split(':')[-1]) for i in contents]

val_total_step = []

for i in range(len(val_step)):
    val_total_step.append(steps_per_epoch*val_epoch[i]+val_step[i])


train_epochs = np.array(train_total_step)/steps_per_epoch
val_epochs = np.array(val_total_step)/steps_per_epoch


plt.figure(figsize=(8,6))
plt.plot(train_epochs,train_loss, label='Training')
plt.plot(val_epochs, val_loss, label='Validation')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()


train_epochs = np.array(train_total_step[49:-50])/steps_per_epoch
val_epochs = np.array(val_total_step[2:-3])/steps_per_epoch

plt.figure(figsize=(8,6))
plt.plot(train_epochs, moving_average(train_loss,100), label='Training')
plt.plot(val_epochs, moving_average(val_loss,6), label='Validation')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()

"""### Model performance and confusion matrix"""

# Loading finetuned model
run_name = "Final_lr=5e-6"
model_name = 'codebert-base'
dir_name = 'codebert_finetune_runs/{}'.format(run_name)
model = AutoModelForSequenceClassification.from_pretrained("codebert_finetune_runs/Final_lr=5e-6/epoch_5").to(device)

test_acc = []
test_loss_total = 0
(X_train, A_train, Y_train), (X_val, A_val, Y_val), (X_test, A_test, Y_test) = split_loader(dir_name)

pred_list = []
truth_list = []

for test_badtch_id, j in tqdm(enumerate(range(0, X_test.shape[0]))):
                    # Loading singular validation data (overwrites train data as can only load 1 intp GPU)
                    batch_X, batch_Y, batch_A = X_test[j:j+1].to(device), Y_test[j:j+1].to(device), A_test[j:j+1].to(device)

                    with torch.no_grad():
                        test_outputs = model(batch_X, labels=batch_Y, attention_mask=batch_A)
                    test_loss_total += float(test_outputs['loss'].item())

                    
                    test_clsf = nn.Softmax(dim=1)(test_outputs.logits)

                    pred_list.append(test_clsf.argmax(axis=1).cpu()[0].item())
                    truth_list.append(batch_Y.cpu()[0].item())

                    test_acc.append(np.average(torch.eq(batch_Y.cpu(), test_clsf.argmax(axis=1).cpu())))

test_loss_total /= (j+1)
test_acc = np.average(test_acc)

from sklearn.metrics import confusion_matrix

tn, fp, fn, tp = confusion_matrix(truth_list, pred_list).ravel()

print("\n\nConfusion Matrix:")
print("TP: {}, FP:{}\nFN:{}, TN:{}".format(tp, fp, fn, tn))
print("\nAccuracy {:.4f}".format(test_acc))
print("Precision: {:.4f}".format(tp/(tp+fp)))
print("Recall: {:.4f}".format(tp/(tp+fn)))

"""## Modified BERT architechture with additional FNN layers

Bellow there are experiments with a modified BERT architechture, this has not been cleaned as the results were worse than above. For similar reasons it was not included within the report
"""

class BERT_Arch(nn.Module):

    def __init__(self, bert):
      
      super(BERT_Arch, self).__init__()

      self.bert = bert 
      
      # dropout layer
      self.dropout = nn.Dropout(0.1)
      
      # relu activation function
      self.relu =  nn.ReLU()

      # dense layer 1
      self.fc1 = nn.Linear(512,512)
      
      # dense layer 2 (Output layer)
      self.fc2 = nn.Linear(512,2)

      #softmax activation function
      self.softmax = nn.LogSoftmax(dim=1)

    #define the forward pass
    def forward(self, sent_id, mask):

      #pass the inputs to the model  
      cls_hs = self.bert(sent_id, attention_mask=mask)

      x = self.fc1(cls_hs.logits)

      x = self.relu(x)

      x = self.dropout(x)

      # output layer
      x = self.fc2(x)
      
      # apply softmax activation
      x = self.softmax(x)

      return x

"""
Main configuration function for a given finetune run
:return: None
"""
run_name = "lr_5e-5 redo, 512 split Full unfreeze"
run_dir = "codebert_finetune_runs/{}".format(run_name)
model_name = 'codebert-base'
checkpoint_location = None
online = False
load_splits = False
save_data = True

print("generating data splits")

code_df = preprocess_data(file_loc='code_dataset.jsonl')
train_data, val_data, test_data = tokenize(code_df, model_name=model_name)

X_train, A_train, Y_train = train_data
X_val, A_val, Y_val = val_data
X_test, A_test, Y_test = test_data

data_type = ['train', 'val', 'test']
data_split_type = ['X', 'A', 'Y']


# Creating dir to save logs and checkpoints, re
dir_name = "{}".format(run_dir)
if os.path.exists(dir_name):
  input("run name already exists, press Enter to overwrite")
else:
  os.makedirs(dir_name)

if save_data:
  print("saving data splits")

  data_all = [train_data, val_data, test_data]
  for i, data in enumerate(data_all):
    for j, split in enumerate(data):
      with open('{}/{}_{}.pickle'.format(run_dir,data_type[i], data_split_type[j]), 'wb') as handle:
        pickle.dump(split, handle)

# Loading model from checkpoint if location provided
if online:
  print("loading model from online")
  model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base")
elif checkpoint_location is None:
  print("loading model from local repo")

  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=512)
else:
  print("loading model from checkpoint: {}".format(checkpoint_location))
  model = AutoModelForSequenceClassification.from_pretrained(checkpoint_location)

# freeze all the parameters
for param in model.parameters():
    param.requires_grad = False

custom_model = BERT_Arch(model)

custom_model = custom_model.to(device)

def train_custom(model, train_data, val_data, epochs=5, batch_size=16, learning_rate=2e-5, validate_per=500,
          run_name="temp", run_descrption=None):
    """
    Main fine-tuning training loop for the provided model

    :param model: model loaded with predefined weights
    :param train_data: tuple of X_train, A_train, Y_train (X = inputs, A = attention, Y = target)
    :param val_data: tuple X_val, A_val, Y_val
    :param epochs: number of epochs for training
    :param batch_size: batch size (see note below about batch_hack)
    :param learning_rate: optimizer learning rate
    :param validate_per: number of weight updates before validation occurs
                            (notes: - if batch_size = 32, and validate_per = 32, validation will occur every batch
                                    - this is wrt the start of each epoch
                                    - validation will always occour at the start of each epoch (step 0))
    :param run_name: name used to saving checkpoints and log files within codebert_finetune_runs
    :param run_descrption: string that is saved to info.txt describing the run


    :return: None (models are saved in checkpoints along with log data)
    """


    # Saving run description.txt
    if run_descrption is not None:
        with open("{}/info.txt".format(dir_name), "a+") as f:
            f.write(run_descrption)

    # Unpacking data
    X_train, A_train, Y_train = train_data
    X_val, A_val, Y_val = val_data


    batch_hack = batch_size  # See note below regarding limited GPU memory

    # Initializing arrays for tracking loss
    train_loss_hist = []
    val_loss_hist = []
    train_pred_hist = []
    # Counter to track batches (see note below related to GPU memory)
    batch_count = 0
    # validate_per_batch = int(validate_per/batch_hack)

    # Moving model to GPU if configured
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    validate_per = int(validate_per/batch_size)

    cross_entropy  = nn.NLLLoss() 
    for epoch in range(epochs):

        # Generating random index for manual shuffling of data each epoch as note using DataLoaders
        permutation = torch.randperm(X_train.shape[0])

        # Note here that only a single element is loaded at each iteration (batch size = 1) due to GPU memory constraint
        for batch_id, i in enumerate(range(0, X_train.shape[0], batch_hack)):

            # Loading batch and moving to device
            indices = permutation[i:i + batch_hack]
            batch_X, batch_Y, batch_A = X_train[indices].to(device), Y_train[indices].to(device), A_train[indices].to(device)


            batch_Y_one_hot = torch.nn.functional.one_hot(batch_Y, num_classes= 2)

            model.train()

            # Forward pass
            #outputs = model(batch_X,labels=batch_Y, attention_mask=batch_A)
            outputs = model(batch_X, batch_A)

            #loss = criterion(loss_clsf.float(), batch_Y_one_hot.float())
            #loss = outputs.loss

            loss = cross_entropy(outputs, batch_Y)


            # Clip params
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)


            # Backward pass
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            loss_clsf = nn.Softmax(dim=1)(outputs)
            acc = np.average(torch.eq(batch_Y.cpu(), loss_clsf.argmax(axis=1).cpu()))
            #rint(correct)

            # Tracking loss
            train_loss_hist.append(float(loss.item()))
            train_pred_hist.append(acc)

            # Training output
            train_output = "Epoch:{} Step:{} Training_loss:{:.6f}, Acc_avg:{:.2f}%".format(epoch, i, loss.item(), np.sum(100*train_pred_hist[-50:])/min(len(train_pred_hist), 50))
            print(train_output+" Training_loss_avg:{:.6f}".format(np.average(train_loss_hist[-50:])))
            with open("{}/train_loss.txt".format(dir_name), "a+") as f:
                f.write(train_output+"\n")

            # Validation
            if batch_id % validate_per == 0:
                val_loss_total = 0
                model.eval()
                print("Validating:")
                val_acc = []
                for val_badtch_id, j in tqdm(enumerate(range(0, X_val.shape[0], batch_hack))):
                    # Loading singular validation data (overwrites train data as can only load 1 intp GPU)
                    batch_X, batch_Y, batch_A = X_val[j:j+batch_hack].to(device), Y_val[j:j+batch_hack].to(device), A_val[j:j+batch_hack].to(device)

                    with torch.no_grad():
                        outputs = model(batch_X, batch_A)

                    val_loss = cross_entropy(outputs, batch_Y)
                    val_loss_total += float(val_loss)

                    
                    val_clsf = nn.Softmax(dim=1)(outputs)
                    val_acc.append(np.average(torch.eq(batch_Y.cpu(), val_clsf.argmax(axis=1).cpu())))

                    del batch_X
                    del batch_Y

                # Adding average loss to tracker
                val_average = val_loss_total / (val_badtch_id+1)
                val_loss_hist.append(val_average)

                # Validation output and logging
                val_output = "Epoch:{} Step:{} Val_loss:{:.6f}, Val_Acc_avg:{:.2f}%".format(epoch, i, val_average, np.sum(100*val_acc[-50:])/min(len(val_acc), 50))
                print(val_output)
                with open("{}/val_los.txt".format(dir_name), "a+") as f:
                    f.write(val_output+"\n")

        # End of epoch checkpoint
        #model.save_pretrained("{}/epoch_{}".format(dir_name, epoch + 1))
        torch.save(model, "{}/epoch_{}".format(dir_name, epoch + 1))

train_custom(model=custom_model,
      train_data=train_data,
      val_data=val_data,
      epochs=10,
      batch_size=8,
      learning_rate=5e-5,
      validate_per=250,
      run_name=run_name,
      run_descrption="lr_5e-5 redo")