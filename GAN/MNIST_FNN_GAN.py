# -*- coding: utf-8 -*-
"""HW3_Q4_SamDonald.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/118nhQ8LJ3R4l0WhniNwZAo3FdGjr62WJ

# CS5814: Assignment 3

## Problem 4

This problem involves creating a generative adversarial network is to replicate digits from the MNIST dataset.

Two different configurations are tested, both based on a simple FNN architecture for the generator and discriminator.

## Imports and config
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data_utils
from torchvision import transforms, datasets
from torchvision.utils import make_grid, save_image
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import numpy as np
import random
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

"""## Configure Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My Drive/CS5814/HW3/Q4

dir = "GAN_images_2"

!mkdir GAN_images_2

"""## MNIST Dataset download and configuration"""

batch_size = 512

transform = transforms.Compose([transforms.ToTensor(), #])
                               transforms.Normalize((0.5,), (0.5,))]) # for normalzing between -1,1])

# Loading data
MNIST_data_train = datasets.MNIST("", train=True, download=True,transform=transform)
MNIST_data_test = datasets.MNIST("", train=False, download=True,transform=transform)

"""# GAN Model Definition
## Generator
"""

class Generator(nn.Module):
    def __init__(self, input_width=10,
                 lw=(784, 784, 784),
                 dropout_r=0,
                 activation_r=0.2,
                 output_a="Tanh"):
        super().__init__()

        # Activiation functions
        # Note this is directly related to the transform used on the dataset
        #   as they need to match for the current plotting config
        a = nn.LeakyReLU(activation_r)

        if output_a == "Sigmoid":
          output_act = nn.Sigmoid()  
        elif output_a == "Tanh":
          output_act = nn.Tanh()

        # Dropout
        d = nn.Dropout(dropout_r)

        # Layer definitions
        self.input_dim = input_width
        l1 = nn.Linear(self.input_dim, lw[0])
        l2 = nn.Linear(lw[0], lw[1])
        l3 = nn.Linear(lw[1], lw[2])
        l4 = nn.Linear(lw[2], 784)
        
        layers = [l1, a, d, l2, a, d, l3, a, d, l4, output_act]

        self.module_list = nn.ModuleList(layers)

    def forward(self, x):
        for f in self.module_list:
            x = f(x)

        x = x.view(-1,1,28,28)
        return x

"""## Discriminator"""

class Discriminator(nn.Module):
    def __init__(self, input_width=784, 
                 lw=(784, 784, 784),
                 dropout_r=0,
                 activation_r=0.2,
                 output_a="Sigmoid"):
        super().__init__()

        # Activiation functions
        # Note this is directly related to the transform used on the dataset
        a = nn.LeakyReLU(activation_r)
        output_act = nn.Sigmoid()  


        # Dropout
        d = nn.Dropout(dropout_r)

        # Layer definitions
        self.input_dim = input_width
        l1 = nn.Linear(self.input_dim, lw[0])
        l2 = nn.Linear(lw[0], lw[1])
        l3 = nn.Linear(lw[1], lw[2])
        l4 = nn.Linear(lw[2], 1)

        layers = [l1, a, d, l2, a, d, l3, a, d, l4, output_act]

        self.module_list = nn.ModuleList(layers)

    def forward(self, x):
        x = x.view(-1, self.input_dim)
        for f in self.module_list:
            x = f(x)
        return x

"""## Supporting functions"""

def generate_noise(sample_size, noise_size):
    return torch.randn(sample_size, noise_size).to(device)

def patcher(img, epoch):
    """
    Hack to patch tensors into single image, plot and save.
    """

    imgs = torch.reshape(img,(-1,28,28))
    rows = []
    w = 8

    for row_id in range(w):
        row = imgs[w * row_id]
        for col_id in range(1,w):
            row = torch.hstack((row, imgs[row_id*w+col_id]))
        rows.append(row)

    output_img = rows[0]
    for row_id in range(1,w):
        output_img = torch.vstack((output_img, rows[row_id]))

    plt.imshow(output_img,cmap='gray')
    plt.axis('off')
    plt.savefig("{}/epoch_{}.png".format(dir, str(epoch).zfill(3)),
                bbox_inches='tight',
                dpi=600)
    
    plt.clf()

"""## Training subloops
### Generator
"""

def train_generator(optimizer, data_fake):
    optimizer.zero_grad()

    # Infer the batch size to generate real labels
    batch_size = data_fake.size(0)
    real_label = torch.ones(batch_size, 1).to(device)

    # Pass generate data to discriminator
    output_D = discriminator(data_fake)
    loss_G = criterion(output_D, real_label)
    loss_G.backward()

    optimizer.step()

    return loss_G

"""### Discriminator"""

def train_discriminator(optimizer, data_real, data_fake):
    optimizer.zero_grad()

    # Infer the batch size to generate real and fake labels
    batch_size = data_real.size(0)
    real_label = torch.ones(batch_size, 1).to(device)
    fake_label = torch.zeros(batch_size, 1).to(device)

    # Real forward and backward pass
    output_real = discriminator(data_real)
    loss_real = criterion(output_real, real_label)

    # Fake forward and backward pass
    output_fake = discriminator(data_fake)
    loss_fake = criterion(output_fake, fake_label)

    loss_real.backward()
    loss_fake.backward()

    optimizer.step()

    # Collate loss
    total_loss = loss_real + loss_fake
    return total_loss

"""## Configuring Traning Instance"""

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Learning parameters
batch_size = 512
epochs = 100
num_steps_D = 1 # number of steps to apply to the discriminator training
noise_dim = 10

# Creating GAN
generator = Generator(input_width=noise_dim).to(device)
discriminator = Discriminator().to(device)
generator.train()
discriminator.train()
print(generator)
print(discriminator)

# Optimizers and loss function
optim_g = optim.Adam(generator.parameters(), lr=0.0005)
optim_d = optim.Adam(discriminator.parameters(), lr=0.0005)
criterion = nn.BCELoss()

# Generating DataLoaders
train_dl = torch.utils.data.DataLoader(MNIST_data_train,
                                       batch_size=batch_size,
                                       shuffle=True)

test_dl = torch.utils.data.DataLoader(MNIST_data_train,
                                       batch_size=batch_size,
                                       shuffle=False)

# Training noise to display progress
# Noise_size needs to be the same size as the input to generator
# 64 as tracking 64 total images (8x8)
noise = generate_noise(sample_size=64, noise_size=noise_dim)

# Tracking arrays
losses_g = []
losses_d = []
images = []
loss_train_G = []
loss_train_D = []

"""### Traning Loop"""

for epoch in range(epochs):

    # Tracking losses for epoch
    loss_D = 0
    loss_G = 0

    for i, data in enumerate(train_dl):
        data_real = data[0].to(device)  # Don't need the label
        batch_size = data_real.shape[0]

        # Train discriminator
        for j in range(num_steps_D):
          data_fake = generator(generate_noise(batch_size, noise_dim)).detach()
          loss_D += train_discriminator(optim_d, data_real, data_fake).item()

        # Train generator
        data_fake = generator(generate_noise(batch_size, noise_dim))
        loss_G += train_generator(optim_g, data_fake).item()

    # Display generate image from noise for the epoch
    generator.eval()
    generated_img = generator(noise).cpu().detach()
    generator.train()

    # Saving image
    patcher(generated_img, epoch)

    # Calculating average losses based on step counts
    loss_G /= (i+1)
    loss_D /= ((j+1)*(i+1))

    # Tracking losses and printing information
    loss_train_G.append(loss_G)
    loss_train_D.append(loss_D)
    print("Epoch:{}, Loss_G:{}, Loss_D:{}".format(epoch, loss_G, loss_D))

# Saving models (could be done with .state_dict())
torch.save(generator, "{}/generator".format(dir))
torch.save(discriminator, "{}/discriminator".format(dir))

"""## Post training

### Plotting
"""

plt.figure(figsize=(8,6))
plt.plot(loss_train_G, label='Generator')
plt.plot(loss_train_D, label='Discriminator')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()
plt.show()

"""## Evaluating performance"""

# Setting random seeds
RANDOM_SEED = 3819969

torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

generator.eval()
discriminator.eval()

for i, real_img_data in enumerate(test_dl):

        real_img_X = real_img_data[0].to(device)  # Don't need the label

        real_preds = discriminator(real_img_X).cpu().detach().numpy()
        batch_size = real_img_X.size(0)
        real_label = torch.ones(batch_size, 1).numpy()
        # Pass generate data to discriminator
        output_D = discriminator(data_fake)

        if i == 0:
          break
test_noise = generate_noise(512, noise_dim)
fake_imag_X = generator(test_noise)
fake_label = torch.zeros(batch_size, 1).numpy()
fake_preds = discriminator(fake_imag_X).cpu().detach().numpy()

real_preds = (real_preds >= 0.5).astype(int)
fake_preds = (fake_preds >= 0.5).astype(int)

net_preds = np.concatenate((real_preds, fake_preds))
net_label = np.concatenate((real_label, fake_label))

tn, fp, fn, tp = confusion_matrix(net_label, net_preds).ravel()

test_acc = accuracy_score(net_label, net_preds)


print("\n\nConfusion Matrix:")
print("TP: {}, FP:{}\nFN:{}, TN:{}".format(tp, fp, fn, tn))
print("\nAccuracy {:.4f}".format(test_acc))
print("Precision: {:.4f}".format(tp/(tp+fp)))
print("Recall: {:.4f}".format(tp/(tp+fn)))
print("F1: {:.4f}".format(tp/(tp+0.5*(fp+fn))))

"""## Improving Generator and Discriminator"""

dir = "GAN_images_5"

!mkdir GAN_images_5

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Learning parameters
batch_size = 512
epochs = 300
num_steps_D = 3 # number of steps to apply to the discriminator training
num_steps_G = 1
noise_dim = 64
lr = 0.0002
# Creating GAN

generator = Generator(input_width=noise_dim)
                      
discriminator = Discriminator()


generator = Generator(input_width=noise_dim,
                      lw=(256,512,1024),
                      output_a="Tanh")     
          
discriminator = Discriminator(lw=(1024,512,256),
                      dropout_r=0.3)


generator = generator.to(device)
discriminator = discriminator.to(device)
generator.train()
discriminator.train()
print(generator)
print(discriminator)

# Optimizers and loss function
optim_g = optim.Adam(generator.parameters(), lr=lr)
optim_d = optim.Adam(discriminator.parameters(), lr=lr)
criterion = nn.BCELoss()

# Generating DataLoaders
train_dl = torch.utils.data.DataLoader(MNIST_data_train,
                                       batch_size=batch_size,
                                       shuffle=True)

test_dl = torch.utils.data.DataLoader(MNIST_data_train,
                                       batch_size=batch_size,
                                       shuffle=False)

# Training noise to display progress
# Noise_size needs to be the same size as the input to generator
# 64 as tracking 64 total images (8x8)
noise = generate_noise(sample_size=64, noise_size=noise_dim)

# Tracking arrays
losses_g = []
losses_d = []
images = []
loss_train_G = []
loss_train_D = []

for epoch in range(epochs):

    # Tracking losses for epoch
    loss_D = 0
    loss_G = 0

    for i, data in enumerate(train_dl):
        data_real = data[0].to(device)  # Don't need the label
        batch_size = data_real.shape[0]

        # Train discriminator
        for j in range(num_steps_D):
          data_fake = generator(generate_noise(batch_size, noise_dim)).detach()
          loss_D += train_discriminator(optim_d, data_real, data_fake).item()

        # Train generator
        for k in range(num_steps_G):
          data_fake = generator(generate_noise(batch_size, noise_dim))
          loss_G += train_generator(optim_g, data_fake).item()

    # Display generate image from noise for the epoch
    generator.eval()
    generated_img = generator(noise).cpu().detach()
    generator.train()

    # Saving image
    patcher(generated_img, epoch)

    # Calculating average losses based on step counts
    loss_G /= ((k+1)*(i+1))
    loss_D /= ((j+1)*(i+1))

    # Tracking losses and printing information
    loss_train_G.append(loss_G)
    loss_train_D.append(loss_D)
    print("Epoch:{}, Loss_G:{}, Loss_D:{}".format(epoch, loss_G, loss_D))

# Saving models (could be done with .state_dict())
torch.save(generator, "{}/generator".format(dir))
torch.save(discriminator, "{}/discriminator".format(dir))

"""## Post training 
### Plotting
"""

plt.figure(figsize=(8,6))
plt.plot(loss_train_G, label='Generator')
plt.plot(loss_train_D, label='Discriminator')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()
plt.show()

"""## Evaluating performance"""

# Setting random seeds
RANDOM_SEED = 3819969

torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

generator.eval()
discriminator.eval()

for i, real_img_data in enumerate(test_dl):

        real_img_X = real_img_data[0].to(device)  # Don't need the label

        real_preds = discriminator(real_img_X).cpu().detach().numpy()
        batch_size = real_img_X.size(0)
        real_label = torch.ones(batch_size, 1).numpy()
        # Pass generate data to discriminator
        output_D = discriminator(data_fake)

        if i == 0:
          break
test_noise = generate_noise(512, noise_dim)
fake_imag_X = generator(test_noise)
fake_label = torch.zeros(batch_size, 1).numpy()
fake_preds = discriminator(fake_imag_X).cpu().detach().numpy()

real_preds = (real_preds >= 0.5).astype(int)
fake_preds = (fake_preds >= 0.5).astype(int)

net_preds = np.concatenate((real_preds, fake_preds))
net_label = np.concatenate((real_label, fake_label))

tn, fp, fn, tp = confusion_matrix(net_label, net_preds).ravel()

test_acc = accuracy_score(net_label, net_preds)

print("\n\nConfusion Matrix:")
print("TP: {}, FP:{}\nFN:{}, TN:{}".format(tp, fp, fn, tn))
print("\nAccuracy {:.4f}".format(test_acc))
print("Precision: {:.4f}".format(tp/(tp+fp)))
print("Recall: {:.4f}".format(tp/(tp+fn)))
print("F1: {:.4f}".format(tp/(tp+0.5*(fp+fn))))

"""## Continuing training for output video!"""

generator.train()
discriminator.train()

for epoch in range(300, epochs+300, 1):

    # Tracking losses for epoch
    loss_D = 0
    loss_G = 0

    for i, data in enumerate(train_dl):
        data_real = data[0].to(device)  # Don't need the label
        batch_size = data_real.shape[0]

        # Train discriminator
        for j in range(num_steps_D):
          data_fake = generator(generate_noise(batch_size, noise_dim)).detach()
          loss_D += train_discriminator(optim_d, data_real, data_fake).item()

        # Train generator
        for k in range(num_steps_G):
          data_fake = generator(generate_noise(batch_size, noise_dim))
          loss_G += train_generator(optim_g, data_fake).item()

    # Display generate image from noise for the epoch
    generator.eval()
    generated_img = generator(noise).cpu().detach()
    generator.train()

    # Saving image
    patcher(generated_img, epoch)

    # Calculating average losses based on step counts
    loss_G /= ((k+1)*(i+1))
    loss_D /= ((j+1)*(i+1))

    # Tracking losses and printing information
    loss_train_G.append(loss_G)
    loss_train_D.append(loss_D)
    print("Epoch:{}, Loss_G:{}, Loss_D:{}".format(epoch, loss_G, loss_D))

# Saving models (could be done with .state_dict())
torch.save(generator, "{}/generator".format(dir))
torch.save(discriminator, "{}/discriminator".format(dir))